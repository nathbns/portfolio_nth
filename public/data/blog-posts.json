[
  {
    "title": "Dijkstra's algorithm (/ˈdaɪkstrəz/ DYKE-strəz)...",
    "date": "2025-08-29",
    "path": "dsa_algos/dijkstra.md",
    "category": "dsa_algos",
    "content": "Dijkstra's algorithm (/ˈdaɪkstrəz/ DYKE-strəz) is an algorithm for finding the shortest paths between nodes in a weighted graph, which may represent, for example, a road network. It was conceived by computer scientist Edsger W. Dijkstra in 1956 and published three years later. (https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm)\n\n**When to use a Dijkstra's algorithm?**\n- Dijkstra's algorithm finds the shortest path from a given source node to every other node.\n- It can be used to find the shortest path to a specific destination node, by terminating the algorithm after determining the shortest path to the destination node.\n\n**C++ Implementation Example on 743. Network Delay Time:**\n\n![image](dijkstra_img.jpeg)\n\n```c++\n#include \u003ciostream\u003e\n#include \u003cvector\u003e\n#include \u003cunordered_map\u003e\n#include \u003cqueue\u003e\n\nusing namespace std;\n\nclass DjikstraGraph {\npublic:\n    unsigned int networkDelayTime(vector\u003cvector\u003cint\u003e\u003e\u0026 times, int n, int k) {\n        unordered_map\u003cint, vector\u003cpair\u003cint, int\u003e\u003e\u003e graph;\n        for (auto\u0026 time : times) {\n            graph[time[0]].push_back({time[1], time[2]});\n        }\n\n        vector\u003cint\u003e dist(n + 1, INT_MAX);\n        dist[k] = 0;\n\n        priority_queue\u003cpair\u003cint, int\u003e, vector\u003cpair\u003cint, int\u003e\u003e, greater\u003cpair\u003cint, int\u003e\u003e\u003e pq;\n        pq.push({0, k});\n\n        while (!pq.empty()) {\n            auto [d, node] = pq.top();\n            pq.pop();\n\n            if (d \u003e dist[node]) continue;\n\n            for (auto\u0026 edge : graph[node]) {\n                int neighbor = edge.first;\n                int weight = edge.second;\n\n                if (dist[node] + weight \u003c dist[neighbor]) {\n                    dist[neighbor] = dist[node] + weight;\n                    pq.push({dist[neighbor], neighbor});\n                }\n            }\n        }\n\n        int maxTime = 0;\n        for (int i = 1; i \u003c= n; i++) {\n            if (dist[i] == INT_MAX) return -1;\n            maxTime = max(maxTime, dist[i]);\n        }\n\n        return maxTime;\n    }\n};\n\nint main(void) {\n    DjikstraGraph djG;\n    vector\u003cvector\u003cint\u003e\u003e time = {{2,1,1},{2,3,1},{3,4,1}};\n    unsigned int ans = djG.networkDelayTime(time, 4, 2);\n    cout\u003c\u003c\"ans =\u003e \"\u003c\u003cans\u003c\u003cendl;\n    return 0;\n}\n```\n\n**Complexity**\n\n- O(V^2)\n"
  },
  {
    "title": "BPE(byte-pair encoding): Use in GPT | GPT2",
    "date": "2025-08-17",
    "path": "tokenizer/bpe.md",
    "category": "tokenizer",
    "content": "## BPE(byte-pair encoding): Use in GPT | GPT2\n\n```python\nfrom transformers import AutoTokenizer \n\ntokenizer = AutoTokenizer.from_pretrained('gpt2')\n\nfrom collections import defaultdict\n\ncorpus = [\"This is an introduction course.\", \"This is about tokenization.\", \"This section shows multiple tokenizer algorithms.\", \"Hope you like the content so far.\"]\n\n# calculate the freq foreach words in the corpus\n\nword_freqs = defaultdict(int)\n\nfor txt in corpus:\n    word_with_offset = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(txt)\n    new_word = [word for word, _ in word_with_offset]\n    for w in new_word:\n        word_freqs[w] += 1\n\nprint(word_freqs)\n\n# vocab base on the corpus (set())\n\n# word_freqs.keys()\nalphabet = []\n\nfor word in word_freqs:\n    for c in word:\n        if c not in alphabet:\n            alphabet.append(c)\n        \nalphabet.sort()\nprint(f\"alphabet: {alphabet}\\n\")\n\n# we add the 'special' char -\u003e \"\u003c|endoftext|\u003e\"\nvocab = [\"\u003c|endoftext|\u003e\"] + alphabet.copy()\nprint(f\"vocab: {vocab}\")\n\n# for training we need foreach word to decompose in each char\nsplits = {word: [c for c in word] for word in word_freqs.keys()}\n\n# function calcule freq of each pairs\n\ndef compute_pair_freqs(splits):\n    pair_freqs = defaultdict(int)\n    for word, freq in word_freqs.items():\n        split = splits[word]\n        if len(split) == 1:\n            continue\n        for i in range(len(split) - 1):\n            pair = (split[i], split[i+1])\n            pair_freqs[pair] += freq\n    return pair_freqs\n\n# just checking our new dict\n# compute_pairs_freq(split)\npair_freqs = compute_pair_freqs(splits)\n\nfor i, key in enumerate(pair_freqs.keys()):\n    print(f\"{key}: {pair_freqs[key]}\")\n    if i \u003e= 5:\n        break\n\n# finding the best pair\nbest_pair = \"\"\nmax_freq = None\n\nfor pair, freq in pair_freqs.items():\n    if max_freq is None or max_freq \u003c freq:\n        max_freq = freq\n        best_pair = pair\n\nprint(f\"best_pair = {best_pair} \u0026\u0026 max_freq = {max_freq}\")\n\n# So the first fusion to learn is ==\u003e ('Ġ', 't') -\u003e 'Ġt' \u0026\u0026 we add 'Ġt' to the vocab\n\n# we have to merge the pair learn by our BPE tokenizer\n\ndef merge_pair(a, b, splits):\n    for word in word_freqs:\n        split = splits[word]\n        if len(split) == 1:\n            continue\n        \n        i = 0\n        while i \u003c len(split) - 1:\n            if split[i] == a and split[i+1] == b:\n                split = split[:i] + [a + b] + split[i+2:]\n            else:\n                i += 1\n        splits[word] = split\n    return splits\n\n\ntarget_vocab_size = 100\nmerges = {}\n\nwhile len(vocab) \u003c target_vocab_size:\n    # calculate the pair freq\n    pair_freqs = compute_pair_freqs(splits)\n    # find the best freq pair\n    best_pair = \"\"\n    max_freq = None\n    for pair, freq in pair_freqs.items():\n        if max_freq is None or max_freq \u003c freq:\n            max_freq = freq\n            best_pair = pair\n    # merging the pair in the corpus\n    splits = merge_pair(*best_pair, splits)\n    \n    merges[best_pair] = best_pair[0] + best_pair[1]\n    \n    # add the new vocab\n    vocab.append(best_pair[0] + best_pair[1])\n\nprint(merges)\nprint(vocab)\n\n\"\"\"\n{('i', 's'): 'is', ('t', 'i'): 'ti', ('o', 'n'): 'on', ('T', 'h'): 'Th', ('Th', 'is'): 'This', ('Ġ', 'a'): 'Ġa', ('ti', 'on'): 'tion', ('o', 'u'): 'ou', ('k', 'e'): 'ke', ('Ġ', 's'): 'Ġs', ('Ġ', 'is'): 'Ġis', ('n', 't'): 'nt', ('c', 'tion'): 'ction', ('Ġ', 'c'): 'Ġc', ('Ġt', 'o'): 'Ġto', ('Ġto', 'ke'): 'Ġtoke', ('Ġtoke', 'n'): 'Ġtoken', ('Ġtoken', 'i'): 'Ġtokeni', ('Ġtokeni', 'z'): 'Ġtokeniz', ('Ġa', 'n'): 'Ġan', ('Ġ', 'i'): 'Ġi', ('Ġi', 'nt'): 'Ġint', ('Ġint', 'r'): 'Ġintr', ('Ġintr', 'o'): 'Ġintro', ('Ġintro', 'd'): 'Ġintrod', ('Ġintrod', 'u'): 'Ġintrodu', ('Ġintrodu', 'ction'): 'Ġintroduction', ('Ġc', 'ou'): 'Ġcou', ('Ġcou', 'r'): 'Ġcour', ('Ġcour', 's'): 'Ġcours', ('Ġcours', 'e'): 'Ġcourse', ('Ġa', 'b'): 'Ġab', ('Ġab', 'ou'): 'Ġabou', ('Ġabou', 't'): 'Ġabout', ('Ġtokeniz', 'a'): 'Ġtokeniza', ('Ġtokeniza', 'tion'): 'Ġtokenization', ('Ġs', 'e'): 'Ġse', ('Ġse', 'ction'): 'Ġsection', ('Ġs', 'h'): 'Ġsh', ('Ġsh', 'o'): 'Ġsho', ('Ġsho', 'w'): 'Ġshow', ('Ġshow', 's'): 'Ġshows', ('Ġ', 'm'): 'Ġm', ('Ġm', 'u'): 'Ġmu', ('Ġmu', 'l'): 'Ġmul', ('Ġmul', 'ti'): 'Ġmulti', ('Ġmulti', 'p'): 'Ġmultip', ('Ġmultip', 'l'): 'Ġmultipl', ('Ġmultipl', 'e'): 'Ġmultiple', ('Ġtokeniz', 'e'): 'Ġtokenize', ('Ġtokenize', 'r'): 'Ġtokenizer', ('Ġa', 'l'): 'Ġal', ('Ġal', 'g'): 'Ġalg', ('Ġalg', 'o'): 'Ġalgo', ('Ġalgo', 'r'): 'Ġalgor', ('Ġalgor', 'i'): 'Ġalgori', ('Ġalgori', 't'): 'Ġalgorit', ('Ġalgorit', 'h'): 'Ġalgorith', ('Ġalgorith', 'm'): 'Ġalgorithm', ('Ġalgorithm', 's'): 'Ġalgorithms', ('H', 'o'): 'Ho', ('Ho', 'p'): 'Hop', ('Hop', 'e'): 'Hope', ('Ġ', 'y'): 'Ġy', ('Ġy', 'ou'): 'Ġyou', ('Ġ', 'l'): 'Ġl', ('Ġl', 'i'): 'Ġli', ('Ġli', 'ke'): 'Ġlike', ('Ġt', 'h'): 'Ġth', ('Ġth', 'e'): 'Ġthe', ('Ġc', 'on'): 'Ġcon', ('Ġcon', 't'): 'Ġcont', ('Ġcont', 'e'): 'Ġconte'}\n['\u003c|endoftext|\u003e', '.', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'w', 'y', 'z', 'Ġ', 'is', 'ti', 'on', 'Th', 'This', 'Ġa', 'tion', 'ou', 'ke', 'Ġs', 'Ġis', 'nt', 'ction', 'Ġc', 'Ġto', 'Ġtoke', 'Ġtoken', 'Ġtokeni', 'Ġtokeniz', 'Ġan', 'Ġi', 'Ġint', 'Ġintr', 'Ġintro', 'Ġintrod', 'Ġintrodu', 'Ġintroduction', 'Ġcou', 'Ġcour', 'Ġcours', 'Ġcourse', 'Ġab', 'Ġabou', 'Ġabout', 'Ġtokeniza', 'Ġtokenization', 'Ġse', 'Ġsection', 'Ġsh', 'Ġsho', 'Ġshow', 'Ġshows', 'Ġm', 'Ġmu', 'Ġmul', 'Ġmulti', 'Ġmultip', 'Ġmultipl', 'Ġmultiple', 'Ġtokenize', 'Ġtokenizer', 'Ġal', 'Ġalg', 'Ġalgo', 'Ġalgor', 'Ġalgori', 'Ġalgorit', 'Ġalgorith', 'Ġalgorithm', 'Ġalgorithms', 'Ho', 'Hop', 'Hope', 'Ġy', 'Ġyou', 'Ġl', 'Ġli', 'Ġlike', 'Ġth', 'Ġthe', 'Ġcon', 'Ġcont', 'Ġconte']\n\"\"\" \n```"
  },
  {
    "title": "Normalization :",
    "date": "2025-08-17",
    "path": "tokenizer/normalization.md",
    "category": "tokenizer",
    "content": "# Normalization : \n- cleanup the text (rm accents | spaces | unicode normalization | and others...) \n\n\n```python\nyour_name = \"Nathan\" # replace with your own name :)\nsentence = f\"Hellô my nAme is {your_name}, i'm á freñch computer scIence student.\"\n\nfrom transformers import AutoTokenizer\n\n# we will use this model, \n# because we can comparate easily with his alt model 'bert-base-cased'\nmodel_checkpoint = 'bert-base-uncased' \ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\n# the next line of code, show applied the normalization on our 'sentence'\nprint(f\"{tokenizer.backend_tokenizer.normalizer.normalize_str(sentence)}\\n\")\n\nmodel_checkpoint = 'bert-base-cased'\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\nprint(f\"{tokenizer.backend_tokenizer.normalizer.normalize_str(sentence)}\\n\")\n```\n\nWe see that this model does not applied lowercasing or removing accents.\n\n\n### Each model normalize with his own rule. If you use one, it's you responsability to check the documentation if available."
  },
  {
    "title": "Pre-Tokenizer:",
    "date": "2025-08-17",
    "path": "tokenizer/pre_tokenizer.md",
    "category": "tokenizer",
    "content": "# Pre-Tokenizer:\n- A tokenizer cannot be trained on raw text alone. \n- Instead, we first need to split the texts into small entities, like words. \n\n- A word-based tokenizer can simply split a raw text into words on whitespace and punctuation. Those words will be the boundaries of the subtokens the tokenizer can learn during its training.\n\n```python\nyour_name = \"Nathan\" # replace with your own name :)\nsentence = f\"Hello my name is {your_name}, i'm a french computer science student.\"\n\nfrom transformers import AutoTokenizer\n# to simplify some the call of our model, we will use these functions.\n\ndef tokenizer_function(model_checkpoint):\n    return AutoTokenizer.from_pretrained(model_checkpoint)\n\n#\ndef tokenizer_pre_tokenize_str(tokenizer, sentence):\n    return tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(sentence)\n\nmodel_checkpoint = 'bert-base-uncased'\ntokenizer = tokenizer_function(model_checkpoint)\n\npre_tokenize_sentence = tokenizer_pre_tokenize_str(tokenizer, sentence)\n```\n\nWe can see that with gpt2, each 'new' word start with 'Ġ'.\n\n### Let's try another model again.\n\n```python\nmodel_checkpoint = 'gpt2'\ntokenizer = tokenizer_function(model_checkpoint)\n\npre_tokenize_sentence = tokenizer_pre_tokenize_str(tokenizer, sentence)\ns = \"\"\nfor i, (word, offset) in enumerate(pre_tokenize_sentence):\n    s += word\n    if i \u003c len(pre_tokenize_sentence) - 1:\n        s += ', '\nprint(s) # -\u003e \"Hello, Ġmy, Ġname, Ġis, ĠNathan, ,, Ġi, 'm, Ġa, Ġfrench, Ġcomputer, Ġscience, Ġstudent, .\"\n```\n\n### In conclusion, each model is train to pre-tokenize differently.\n#### Now that we see Normalization and Pre-Tokenization, we can reach the next step of the tokenizer."
  },
  {
    "title": "A segment tree is a binary tree data structure use...",
    "date": "2025-08-11",
    "path": "dsa_algos/segment_tree.md",
    "category": "dsa_algos",
    "content": "A segment tree is a binary tree data structure used for efficient range queries and updates on arrays, such as finding the sum or minimum in a subarray. It is especially useful when you need to perform multiple queries and updates on an array.\n\n**When to use a segment tree?**\n- When you need to answer range queries (like sum, min, max) and also update elements efficiently.\n- Examples: Range sum queries, range minimum/maximum queries, dynamic interval problems.\n\n**Python Implementation Example on Leetcode 307. Range Sum Query - Mutable:**\n\n\n\n```python\nclass NumArray:\n    def __init__(self, nums: List[int], L: int = 0, R: int | None = None):\n        if R is None: \n            R = len(nums) - 1\n        self.L, self.R = L, R\n\n        if L == R:    \n            self.sum   = nums[L]\n            self.left  = None\n            self.right = None\n        else:          \n            M = (L + R) // 2\n            self.left  = NumArray(nums, L, M)\n            self.right = NumArray(nums, M + 1, R)\n            self.sum   = self.left.sum + self.right.sum\n\n    def update(self, index: int, val: int) -\u003e None:\n        if self.L == self.R:\n            self.sum = val\n            return\n        M = (self.L + self.R) // 2\n        if index \u003e M:\n            self.right.update(index, val)\n        else:\n            self.left.update(index, val)\n        self.sum = self.left.sum + self.right.sum\n\n    def sumRange(self, left: int, right: int) -\u003e int:\n        if self.L == left and self.R == right:\n            return self.sum\n        M = (self.L + self.R) // 2\n        if left \u003e M:\n            return self.right.sumRange(left, right)\n        elif right \u003c= M:\n            return self.left.sumRange(left, right)\n        else: \n            return (self.left.sumRange(left, M) + self.right.sumRange(M + 1, right))\n\n# Your NumArray object will be instantiated and called as such:\n# obj = NumArray(nums)\n# obj.update(index,val)\n# param_2 = obj.sumRange(left,right)\n```\n\n**Complexité**\n\n- Build: O(n)\n- **query** (interval): O(log n)\n- **update** (point): O(log n)\n"
  }
]